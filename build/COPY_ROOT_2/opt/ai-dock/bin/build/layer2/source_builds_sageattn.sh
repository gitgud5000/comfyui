#!/bin/bash

# Source builds for ComfyUI - xformers, SageAttention, and infinite-image-browsing
# This script builds libraries from source for optimal performance
set -euo pipefail

build_source_builds_main() {
    echo "üî® Building libraries from source..."
    source "$COMFYUI_VENV/bin/activate"

    uv pip install --upgrade pip

    # echo "üîÑ Rebuilding libraries Torch"
    # build_source_torch_setup
    # build_source_triton
    echo "üîÑ Rebuilding libraries SageAttention"
    build_source_sageattention
    cleanup_build_artifacts
    
    echo "‚úÖ Source builds completed!"
}

cleanup_build_artifacts() {
    echo "üßπ Cleaning up build artifacts..."
    
    # Remove build dependencies
    source "$COMFYUI_VENV/bin/activate"
    uv pip uninstall ninja cmake wheel build setuptools-scm || true
    
    # Clean pip cache
    uv cache clean || true
    pip cache purge || true
    
    # Clean apt cache
    apt-get clean
    rm -rf /var/lib/apt/lists/*
    
    # Remove temp files
    rm -rf /tmp/* /var/tmp/*
    
    echo "‚úÖ Cleanup completed"
}
build_source_sageattention() {
    # Usage: build_source_sageattention "8.9;9.0"  (commas or semicolons both ok)
    local REQ_ARCHS="${1:-8.9;9.0}"
    local ARCHS_SEMI="${REQ_ARCHS//,/;}"   # "8.9;9.0"
    local ARCHS_NUM="${ARCHS_SEMI//./}"    # "89;90"
    echo "üß† Building SageAttention for SMs [${ARCHS_SEMI}]‚Ä¶"

    # ---- Python venv ----
    source "$COMFYUI_VENV/bin/activate" || { echo "Venv missing: $COMFYUI_VENV"; return 1; }

    # ---- Match CUDA to torch.version.cuda if available ----
    local TORCH_CUDA_VER
    TORCH_CUDA_VER="$(python - <<'PY'
import torch, sys
print((torch.version.cuda or "").strip())
PY
)"
    echo "üî•Torch CUDA version: $TORCH_CUDA_VER"
    if [[ -n "$TORCH_CUDA_VER" && -d "/usr/local/cuda-${TORCH_CUDA_VER}" ]]; then
        export CUDA_HOME="/usr/local/cuda-${TORCH_CUDA_VER}"
    else
        export CUDA_HOME="${CUDA_HOME:-/usr/local/cuda}"
    fi
    export PATH="${CUDA_HOME}/bin:${PATH}"
    export LD_LIBRARY_PATH="${CUDA_HOME}/lib64:${CUDA_HOME}/lib64/stubs:${LD_LIBRARY_PATH:-}"

    # ---- Toolchain & compilers (prefer gcc-12, fallback to 11) ----
    uv pip install -U setuptools wheel ninja cmake >/dev/null 2>&1 || true
    if command -v gcc-12 >/dev/null 2>&1 && command -v g++-12 >/dev/null 2>&1; then
        echo "‚úÖ Using GCC 12"
        export CC="$(command -v gcc-12)"
        export CXX="$(command -v g++-12)"
        export CUDAHOSTCXX="$CXX"
    elif command -v gcc-11 >/dev/null 2>&1 && command -v g++-11 >/dev/null 2>&1; then
        echo "‚úÖ Using GCC 11"
        export CC="$(command -v gcc-11)"
        export CXX="$(command -v g++-11)"
        export CUDAHOSTCXX="$CXX"
    elif command -v gcc-10 >/dev/null 2>&1 && command -v g++-10 >/dev/null 2>&1; then
        echo "‚úÖ Using GCC 10"
        export CC="$(command -v gcc-10)"
        export CXX="$(command -v g++-10)"
        export CUDAHOSTCXX="$CXX"
    else
        echo "‚ùå No suitable GCC found! Please install gcc-12, gcc-11, or gcc-10."
        return 1
    fi

    # ---- CUDA/PyTorch build hints ----
    export FORCE_CUDA=1
    export TORCH_CUDA_ARCH_LIST="${ARCHS_SEMI}"                 # e.g. "8.9;9.0"
    export CMAKE_ARGS="-DCMAKE_CUDA_ARCHITECTURES=${ARCHS_NUM}" # e.g. "89;90"
    export CUDAARCHS="${ARCHS_NUM}"                             # belt & suspenders
    export NVCC_FLAGS="-Xptxas -O2"
    export TORCH_CUDA_FLAGS="$NVCC_FLAGS"
    export LDFLAGS="-L${CUDA_HOME}/lib64/stubs ${LDFLAGS:-}"
    export CXXFLAGS="${CXXFLAGS:-} -Wno-error"
    export MAX_JOBS="$(nproc)"
    export CMAKE_BUILD_PARALLEL_LEVEL="$(nproc)"

    # ---- Sanity check ----
    nvcc --version || { echo "nvcc not found (CUDA_HOME=$CUDA_HOME)"; return 1; }
    echo "CUDA_HOME=$CUDA_HOME"
    echo "TORCH_CUDA_ARCH_LIST=$TORCH_CUDA_ARCH_LIST"
    echo "CMAKE_ARGS=$CMAKE_ARGS"

    # ---- Fresh clone + patch ----
    cd /tmp && rm -rf SageAttention
    git clone --depth=1 https://github.com/thu-ml/SageAttention
    cd SageAttention || return 1
    echo "üîß Applying PR #147 patch‚Ä¶"
    curl -sL https://github.com/thu-ml/SageAttention/pull/147.patch | patch -p1

    # ---- Build ----
    echo "üî® Building SageAttention‚Ä¶"
    if uv pip install -v --no-build-isolation . 2>&1 | tee build.log; then
        $COMFYUI_VENV_PIP show sageattention >/dev/null 2>&1 || {
            echo "‚ùå Built but not importable."
            grep -nEi "error:|fatal error:|undefined reference|collect2:" build.log | tail -n80 || tail -n80 build.log
            return 1
        }
        echo "‚úÖ SageAttention build completed."
    else
        echo "‚ùå Build failed."
        grep -nEi "error:|fatal error:|undefined reference|collect2:" build.log | tail -n80 || tail -n80 build.log
        return 1
    fi

    # ---- Cleanup ----
    cd / && rm -rf /tmp/SageAttention
    echo "‚úÖ Done for SMs [${ARCHS_SEMI}]"
}

# build_source_sageattentionx() {
#     # 1) SM list (e.g. "8.9;12.0"), default ‚Üí 8.9;12.0
#     # local GPU_ARCH_LIST="${1:-8.9;9.0}" this uses commas not semicolons bc of patch
#     local GPU_ARCH_LIST="${1:-8.9,9.0}" # this uses commas not semicolons bc of patch, not an error
#     echo "üß† Building SageAttention for SMs [${GPU_ARCH_LIST}]..."

#     # 2) Activate venv and force CUDA-path even on GPU-less host
#     source "$COMFYUI_VENV/bin/activate"
#     export TORCH_CUDA_ARCH_LIST="${GPU_ARCH_LIST}"
#     export FORCE_CUDA=1

#     # 3) Clone fresh
#     cd /tmp && rm -rf SageAttention  # Use /tmp instead of /opt
#     git clone --depth=1 https://github.com/thu-ml/SageAttention
#     cd SageAttention

#     echo "üîß Applying PR #147 patch to setup.py‚Ä¶"
#     curl -sL https://github.com/thu-ml/SageAttention/pull/147.patch \
#         | patch -p1

#     # Ensure build toolchain and CUDA env (works on GPU-less hosts)
#     export CUDA_HOME="${CUDA_HOME:-/usr/local/cuda}"
#     export PATH="${CUDA_HOME}/bin:${PATH}"
#     export LD_LIBRARY_PATH="${CUDA_HOME}/lib64:${CUDA_HOME}/lib64/stubs:${LD_LIBRARY_PATH:-}"
#     export LD_LIBRARY_PATH="$CUDA_HOME/lib64:$CUDA_HOME/lib64/stubs:${LD_LIBRARY_PATH:-}"
#     export LDFLAGS="-L$CUDA_HOME/lib64/stubs"
#     CMAKE_CUDA_ARCHITECTURES="89;90"
#     export CMAKE_BUILD_PARALLEL_LEVEL="$(nproc)"   
#     export NVCC_FLAGS="-Xptxas -O2"
# 	export TORCH_CUDA_FLAGS="$NVCC_FLAGS"
# 	# export NVCC_THREADS=14

#     export CC=/usr/bin/gcc-12
#     export CXX=/usr/bin/g++-12
#     export CUDAHOSTCXX=/usr/bin/g++-12
#     #check if gcc 12 is available
#     if ! command -v /usr/bin/gcc-12 &> /dev/null; then
#         echo "‚ùå gcc-12 not found!"
#         exit 1
#     else
#         echo "‚úÖ gcc-12 found!"
#     fi

#     uv pip install -U setuptools wheel ninja cmake >/dev/null 2>&1 || true
#     echo "üî•LD LIB PATH: $LD_LIBRARY_PATH"
#     # Auto-detect and force parallel build
#     echo "üî® Starting SageAttention build..."
#     nvcc --version || { echo "nvcc not found in PATH"; exit 1; }
#     export MAX_JOBS="$(nproc)"
#     echo "MAX JOBS: $MAX_JOBS"

#     # if python setup.py build_ext --library-dirs=/usr/local/cuda/lib64/stubs 2>&1 | tee build.log; then
#     if uv pip install -v --no-build-isolation .  2>&1 | tee build.log; then
#         # check if sage attention available in environment in pip
#         $COMFYUI_VENV_PIP show sageattention >/dev/null 2>&1 || {
#             echo "‚ùå SageAttention build failed! Package not found in environment."
#             echo "üìã Last 20 lines of build output:"
#             tail -n20 build.log || echo "No build log available"
#             echo "üí° Common fixes:"
#             echo "  - Check CUDA toolkit is installed"
#             echo "  - Verify TORCH_CUDA_ARCH_LIST=${GPU_ARCH_LIST}"
#             echo "  - Ensure sufficient disk space"
#             exit 1
#         }

        
#         echo "‚úÖ SageAttention build completed successfully"
#     else
#         echo "‚ùå SageAttention build failed!"
#         echo "üìã Last 20 lines of build output:"
#         tail -n20 build.log || echo "No build log available"
#         echo "üí° Common fixes:"
#         echo "  - Check CUDA toolkit is installed"
#         echo "  - Verify TORCH_CUDA_ARCH_LIST=${GPU_ARCH_LIST}"
#         echo "  - Ensure sufficient disk space"
#         exit 1
#     fi

#     # 6) Cleanup
#     cd / && rm -rf /tmp/SageAttention

#     echo "‚úÖ SageAttention built for SMs [${GPU_ARCH_LIST}]"
# }

build_source_builds_main "$@"
